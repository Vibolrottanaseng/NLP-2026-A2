{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4e550dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "13b3a53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "669f46a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I.\n",
      "Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into\n",
      "the book her sister was reading, but it had no pictures or\n",
      "conversations in it, “and what is the use of a book,” thought Alice\n",
      "“without pictures or conversations?”\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
      "making a daisy-chain would be worth the trouble of getting up and\n",
      "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
      "close by her.\n",
      "\n",
      "There was nothing so _very_ remarkable in that; nor did Alice think it\n",
      "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n",
      "dear! Oh dear! I shall be late!” (when she thought it over afterwards,\n",
      "it occurred to her that she ought to have wondered at this, but at the\n",
      "time it all seemed quite natural); but when the Rabbit actually _took a\n",
      "watch out of its w\n",
      "Characters: 144018\n",
      "Words: 26441\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/Alice_Adventure_In_Wonderland.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "print(txt[:1000])\n",
    "print(\"Characters:\", len(txt))\n",
    "print(\"Words:\", len(txt.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6708e5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview: chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, and what is the use of a book, thought alice with\n",
      "Num tokens: 26381\n",
      "First 30 tokens: ['chapter', 'i.', 'down', 'the', 'rabbit-hole', 'alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank,', 'and', 'of', 'having', 'nothing', 'to', 'do:', 'once', 'or', 'twice', 'she']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"data/Alice_Adventure_In_Wonderland.txt\"\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)  # normalize whitespace\n",
    "    # keep punctuation that helps generation feel natural\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\.\\,\\!\\?\\;\\:\\'\\-]\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = f.read()\n",
    "\n",
    "text = clean_text(raw)\n",
    "tokens = text.split()\n",
    "\n",
    "\n",
    "print(\"Preview:\", text[:300])\n",
    "print(\"Num tokens:\", len(tokens))\n",
    "print(\"First 30 tokens:\", tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c21f1e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(tokens)\n",
    "train_end = int(0.8 * n)\n",
    "val_end = int(0.9 * n)\n",
    "\n",
    "train_tokens = tokens[:train_end]\n",
    "valid_tokens = tokens[train_end:val_end]\n",
    "test_tokens  = tokens[val_end:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b6a169e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 21104\n",
      "Valid tokens: 2638\n",
      "Test tokens: 2639\n"
     ]
    }
   ],
   "source": [
    "train_dataset = [{\"tokens\": train_tokens}]\n",
    "valid_dataset = [{\"tokens\": valid_tokens}]\n",
    "test_dataset  = [{\"tokens\": test_tokens}]\n",
    "\n",
    "print(\"Train tokens:\", len(train_tokens))\n",
    "print(\"Valid tokens:\", len(valid_tokens))\n",
    "print(\"Test tokens:\", len(test_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d93b22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1554\n"
     ]
    }
   ],
   "source": [
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "EOS = \"<eos>\"\n",
    "MIN_FREQ = 2\n",
    "\n",
    "counter = Counter(train_tokens)\n",
    "\n",
    "vocab_list = [PAD, UNK, EOS] + [\n",
    "    w for w, c in counter.items() if c >= MIN_FREQ\n",
    "]\n",
    "\n",
    "vocab = {w: i for i, w in enumerate(vocab_list)}\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6ae1d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAD = \"<pad>\"\n",
    "# UNK = \"<unk>\"\n",
    "# EOS = \"<eos>\"\n",
    "# MIN_FREQ = 2   # words appearing <2 times become <unk>\n",
    "\n",
    "# counter = Counter(tokens)\n",
    "\n",
    "# vocab = [PAD, UNK, EOS] + [w for w, c in counter.items() if c >= MIN_FREQ]\n",
    "# stoi = {w: i for i, w in enumerate(vocab)}\n",
    "# itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "# ids = [stoi.get(t, stoi[UNK]) for t in tokens]\n",
    "\n",
    "# print(\"Vocab size:\", len(vocab))\n",
    "# print(\"Example ids:\", ids[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ef71af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# with open(\"models/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(\n",
    "#         {\"stoi\": stoi, \"itos\": {str(k): v for k, v in itos.items()}},\n",
    "#         f,\n",
    "#         ensure_ascii=False,\n",
    "#         indent=2\n",
    "#     )\n",
    "\n",
    "# print(\"Saved vocab to models/vocab.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ef44f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size, eos_token=\"<eos>\"):\n",
    "    data = []\n",
    "    unk_id = vocab[\"<unk>\"]\n",
    "\n",
    "    for example in dataset:\n",
    "        if example[\"tokens\"]:\n",
    "            tokens = example[\"tokens\"] + [eos_token]\n",
    "            ids = [vocab.get(tok, unk_id) for tok in tokens]\n",
    "            data.extend(ids)\n",
    "\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "225bb8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: torch.Size([128, 164])\n",
      "Valid data: torch.Size([128, 20])\n",
      "Test data: torch.Size([128, 20])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_data = get_data(train_dataset, vocab, BATCH_SIZE)\n",
    "valid_data = get_data(valid_dataset, vocab, BATCH_SIZE)\n",
    "test_data  = get_data(test_dataset, vocab, BATCH_SIZE)\n",
    "\n",
    "print(\"Train data:\", train_data.shape)\n",
    "print(\"Valid data:\", valid_data.shape)\n",
    "print(\"Test data:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0a825086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode(id_list, itos):\n",
    "#     return \" \".join(itos[i] for i in id_list)\n",
    "\n",
    "# sample_x = x[0].tolist()\n",
    "# sample_y = y[0].tolist()\n",
    "\n",
    "# print(\"X:\", decode(sample_x[:15], itos))\n",
    "# print(\"Y:\", decode(sample_y[:15], itos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa6ded",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "60a94c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        #embedding: [batch size, seq len, emb_dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)      \n",
    "        #output: [batch size, seq len, hid_dim]\n",
    "        #hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        #prediction: [batch size, seq_len, vocab size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e792a79",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "00c9a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "204636e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19,977,746 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5f85c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    #reset the hidden every epoch\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #hidden does not need to be in the computational graph for efficiency\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "        \n",
    "    return epoch_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2846d3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "895caf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m best_valid_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     train_loss = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     valid_loss = evaluate(model, valid_data, criterion, BATCH_SIZE, \n\u001b[32m     13\u001b[39m                 seq_len, device)\n\u001b[32m     15\u001b[39m     lr_scheduler.step(valid_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[39m\n\u001b[32m     21\u001b[39m src, target = src.to(device), target.to(device)\n\u001b[32m     22\u001b[39m batch_size = src.shape[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m prediction, hidden = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m               \n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#need to reshape because criterion expects pred to be 2d and target to be 1d\u001b[39;00m\n\u001b[32m     26\u001b[39m prediction = prediction.reshape(batch_size * seq_len, -\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m#prediction: [batch size * seq len, vocab size]  \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\svrat\\Documents\\2026-Jan\\NLP\\A2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\svrat\\Documents\\2026-Jan\\NLP\\A2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[134]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mLSTMLanguageModel.forward\u001b[39m\u001b[34m(self, src, hidden)\u001b[39m\n\u001b[32m     42\u001b[39m embedding = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.embedding(src))\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m#embedding: [batch size, seq len, emb_dim]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m      \n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m#output: [batch size, seq len, hid_dim]\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m#hidden = h, c = [num_layers * direction, seq len, hid_dim)\u001b[39;00m\n\u001b[32m     47\u001b[39m output = \u001b[38;5;28mself\u001b[39m.dropout(output) \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\svrat\\Documents\\2026-Jan\\NLP\\A2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\svrat\\Documents\\2026-Jan\\NLP\\A2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\svrat\\Documents\\2026-Jan\\NLP\\A2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1141\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1138\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1153\u001b[39m     result = _VF.lstm(\n\u001b[32m   1154\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1155\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1162\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1163\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                BATCH_SIZE, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, BATCH_SIZE, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'models/lm.pt')\n",
    "\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
